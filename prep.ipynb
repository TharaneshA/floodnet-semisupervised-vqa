{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"path/to/FloodNet_Challenge_Track1\"  # Update this path\n",
    "OUTPUT_DIR = r\"C:\\project\\floodnet-semisupervised-vqa\\preprocessed_classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LABELED_FLOODED_PATH = os.path.join(DATA_ROOT, \"Train/Labeled/Flooded/image\")\n",
    "TRAIN_LABELED_NONFLOODED_PATH = os.path.join(DATA_ROOT, \"Train/Labeled/Non-Flooded/image\")\n",
    "TRAIN_UNLABELED_PATH = os.path.join(DATA_ROOT, \"Train/Unlabeled/image\")\n",
    "VALIDATION_PATH = os.path.join(DATA_ROOT, \"Validation/image\")\n",
    "TEST_PATH = os.path.join(DATA_ROOT, \"Test/image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory structure\n",
    "def create_output_structure():\n",
    "    # Main output directory\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Create subdirectories\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, \"train\", \"Flooded\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, \"train\", \"Non-Flooded\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, \"train_unlabeled\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, \"validation\"), exist_ok=True)\n",
    "    os.makedirs(os.path.join(OUTPUT_DIR, \"test\"), exist_ok=True)\n",
    "    \n",
    "    print(f\"Created output directory structure at {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess and save an image\n",
    "def preprocess_and_save(src_path, dest_path, target_size=(224, 224)):\n",
    "    \"\"\"Preprocess image and save to destination path\"\"\"\n",
    "    try:\n",
    "        # Load image\n",
    "        img = Image.open(src_path)\n",
    "        \n",
    "        # Resize to target size\n",
    "        img = img.resize(target_size, Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Save preprocessed image\n",
    "        img.save(dest_path, quality=95)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {src_path}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get all image files from a directory\n",
    "def get_image_files(directory):\n",
    "    \"\"\"Get all image files from a directory\"\"\"\n",
    "    image_files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "            image_files.append(os.path.join(directory, filename))\n",
    "    return image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset():\n",
    "    # Create output structure\n",
    "    create_output_structure()\n",
    "    \n",
    "    # Process labeled flooded images\n",
    "    flooded_images = get_image_files(TRAIN_LABELED_FLOODED_PATH)\n",
    "    print(f\"Processing {len(flooded_images)} flooded images...\")\n",
    "    for src_path in tqdm(flooded_images):\n",
    "        filename = os.path.basename(src_path)\n",
    "        dest_path = os.path.join(OUTPUT_DIR, \"train\", \"Flooded\", filename)\n",
    "        preprocess_and_save(src_path, dest_path)\n",
    "    \n",
    "    # Process labeled non-flooded images\n",
    "    nonflooded_images = get_image_files(TRAIN_LABELED_NONFLOODED_PATH)\n",
    "    print(f\"Processing {len(nonflooded_images)} non-flooded images...\")\n",
    "    for src_path in tqdm(nonflooded_images):\n",
    "        filename = os.path.basename(src_path)\n",
    "        dest_path = os.path.join(OUTPUT_DIR, \"train\", \"Non-Flooded\", filename)\n",
    "        preprocess_and_save(src_path, dest_path)\n",
    "    \n",
    "    # Process unlabeled images\n",
    "    unlabeled_images = get_image_files(TRAIN_UNLABELED_PATH)\n",
    "    print(f\"Processing {len(unlabeled_images)} unlabeled images...\")\n",
    "    for src_path in tqdm(unlabeled_images):\n",
    "        filename = os.path.basename(src_path)\n",
    "        dest_path = os.path.join(OUTPUT_DIR, \"train_unlabeled\", filename)\n",
    "        preprocess_and_save(src_path, dest_path)\n",
    "    \n",
    "    # Process validation images\n",
    "    validation_images = get_image_files(VALIDATION_PATH)\n",
    "    print(f\"Processing {len(validation_images)} validation images...\")\n",
    "    for src_path in tqdm(validation_images):\n",
    "        filename = os.path.basename(src_path)\n",
    "        dest_path = os.path.join(OUTPUT_DIR, \"validation\", filename)\n",
    "        preprocess_and_save(src_path, dest_path)\n",
    "    \n",
    "    # Process test images\n",
    "    test_images = get_image_files(TEST_PATH)\n",
    "    print(f\"Processing {len(test_images)} test images...\")\n",
    "    for src_path in tqdm(test_images):\n",
    "        filename = os.path.basename(src_path)\n",
    "        dest_path = os.path.join(OUTPUT_DIR, \"test\", filename)\n",
    "        preprocess_and_save(src_path, dest_path)\n",
    "    \n",
    "    print(\"Preprocessing complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metadata file with dataset statistics\n",
    "def create_metadata():\n",
    "    \"\"\"Create a metadata file with dataset statistics\"\"\"\n",
    "    metadata = {\n",
    "        \"flooded_count\": len(os.listdir(os.path.join(OUTPUT_DIR, \"train\", \"Flooded\"))),\n",
    "        \"nonflooded_count\": len(os.listdir(os.path.join(OUTPUT_DIR, \"train\", \"Non-Flooded\"))),\n",
    "        \"unlabeled_count\": len(os.listdir(os.path.join(OUTPUT_DIR, \"train_unlabeled\"))),\n",
    "        \"validation_count\": len(os.listdir(os.path.join(OUTPUT_DIR, \"validation\"))),\n",
    "        \"test_count\": len(os.listdir(os.path.join(OUTPUT_DIR, \"test\"))),\n",
    "        \"image_size\": (224, 224),\n",
    "        \"preprocessing\": \"Resized to 224x224 pixels with Lanczos resampling\"\n",
    "    }\n",
    "    \n",
    "    # Save metadata\n",
    "    with open(os.path.join(OUTPUT_DIR, \"metadata.txt\"), \"w\") as f:\n",
    "        for key, value in metadata.items():\n",
    "            f.write(f\"{key}: {value}\\n\")\n",
    "    \n",
    "    print(f\"Metadata saved to {os.path.join(OUTPUT_DIR, 'metadata.txt')}\")\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tfrecord files for faster loading (optional)\n",
    "def create_sample_images():\n",
    "    \"\"\"Create a few sample images to verify preprocessing\"\"\"\n",
    "    sample_dir = os.path.join(OUTPUT_DIR, \"samples\")\n",
    "    os.makedirs(sample_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy a few samples from each category\n",
    "    categories = [\n",
    "        (\"Flooded\", os.path.join(OUTPUT_DIR, \"train\", \"Flooded\")),\n",
    "        (\"Non-Flooded\", os.path.join(OUTPUT_DIR, \"train\", \"Non-Flooded\")),\n",
    "        (\"Unlabeled\", os.path.join(OUTPUT_DIR, \"train_unlabeled\"))\n",
    "    ]\n",
    "    \n",
    "    for category_name, category_path in categories:\n",
    "        files = os.listdir(category_path)\n",
    "        if len(files) > 0:\n",
    "            # Select 3 random files\n",
    "            sample_files = random.sample(files, min(3, len(files)))\n",
    "            \n",
    "            # Copy the sample files\n",
    "            for sample_file in sample_files:\n",
    "                src = os.path.join(category_path, sample_file)\n",
    "                dst = os.path.join(sample_dir, f\"{category_name}_{sample_file}\")\n",
    "                shutil.copy(src, dst)\n",
    "    \n",
    "    print(f\"Sample images copied to {sample_dir}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Print startup message\n",
    "    print(\"FloodNet Image Classification Preprocessing\")\n",
    "    print(\"===========================================\")\n",
    "    print(f\"Source directory: {DATA_ROOT}\")\n",
    "    print(f\"Destination directory: {OUTPUT_DIR}\")\n",
    "    \n",
    "    # Check if directories exist\n",
    "    if not os.path.exists(DATA_ROOT):\n",
    "        print(f\"Error: Source directory {DATA_ROOT} does not exist!\")\n",
    "        print(\"Please update the DATA_ROOT variable in the script.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Process the dataset\n",
    "    process_dataset()\n",
    "    \n",
    "    # Create metadata\n",
    "    metadata = create_metadata()\n",
    "    \n",
    "    # Create sample images\n",
    "    create_sample_images()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\nPreprocessing Summary:\")\n",
    "    print(\"====================\")\n",
    "    for key, value in metadata.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"\\nPreprocessed images are ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
