{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3, ResNet50, Xception\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREPROCESSED_DIR = r\"C:\\project\\floodnet-semisupervised-vqa\\preprocessed_classification\"\n",
    "MODELS_DIR = r\"C:\\project\\floodnet-semisupervised-vqa\\models\"\n",
    "LOGS_DIR = r\"C:\\project\\floodnet-semisupervised-vqa\\logs\"\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (224, 224)\n",
    "EPOCHS = 30\n",
    "STEPS_PER_EPOCH = 20  # As mentioned in the paper\n",
    "LEARNING_RATE = 0.001  # As mentioned in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create necessary directories\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(LOGS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model building function - following exactly the paper description\n",
    "def build_inceptionv3_model():\n",
    "    \"\"\"\n",
    "    Build InceptionV3 model as described in the paper:\n",
    "    - InceptionV3 base with ImageNet weights\n",
    "    - Global Average Pooling\n",
    "    - Fully connected layer with 1024 neurons and ReLU activation\n",
    "    - Output layer with 2 neurons and softmax activation\n",
    "    \"\"\"\n",
    "    # Load pre-trained InceptionV3 model without top layers\n",
    "    base_model = InceptionV3(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(224, 224, 3)\n",
    "    )\n",
    "    \n",
    "    # Add classification layers on top\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    predictions = Dense(2, activation='softmax')(x)  # 2 classes: Flooded and Non-Flooded\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    # Compile model with Adam optimizer and binary cross entropy loss\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model building function for ResNet50 (alternative)\n",
    "def build_resnet50_model():\n",
    "    \"\"\"Build ResNet50 model as described in the paper\"\"\"\n",
    "    base_model = ResNet50(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(224, 224, 3)\n",
    "    )\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    predictions = Dense(2, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model building function for Xception (alternative)\n",
    "def build_xception_model():\n",
    "    \"\"\"Build Xception model as described in the paper\"\"\"\n",
    "    base_model = Xception(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(224, 224, 3)\n",
    "    )\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    predictions = Dense(2, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator setup with augmentation\n",
    "def setup_data_generators():\n",
    "    \"\"\"Set up data generators for training and validation\"\"\"\n",
    "    # Training data generator with augmentation\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    # Validation data generator (only rescaling)\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    # Test data generator (only rescaling)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    # Load training data\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        os.path.join(PREPROCESSED_DIR, 'train'),\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # We don't have labeled validation data in the challenge dataset\n",
    "    # For internal validation, we'll use a separate validation set\n",
    "    # This would need to be adjusted based on your available validation labels\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        os.path.join(PREPROCESSED_DIR, 'train'),  # using train for validation (would need to split in real scenario)\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    return train_generator, val_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semi-supervised learning with pseudo-labeling\n",
    "def generate_pseudo_labels(model, confidence_threshold=0.85):\n",
    "    \"\"\"\n",
    "    Generate pseudo-labels for unlabeled data \n",
    "    using the model's predictions with high confidence\n",
    "    \"\"\"\n",
    "    print(\"Generating pseudo-labels for unlabeled images...\")\n",
    "    \n",
    "    # Directory for unlabeled images\n",
    "    unlabeled_dir = os.path.join(PREPROCESSED_DIR, 'train_unlabeled')\n",
    "    \n",
    "    # Directories for pseudo-labeled images\n",
    "    pseudo_flooded_dir = os.path.join(PREPROCESSED_DIR, 'pseudo_labeled', 'Flooded')\n",
    "    pseudo_nonflooded_dir = os.path.join(PREPROCESSED_DIR, 'pseudo_labeled', 'Non-Flooded')\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(pseudo_flooded_dir, exist_ok=True)\n",
    "    os.makedirs(pseudo_nonflooded_dir, exist_ok=True)\n",
    "    \n",
    "    # Get all unlabeled images\n",
    "    unlabeled_images = [f for f in os.listdir(unlabeled_dir) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    # Counter for pseudo-labels\n",
    "    flooded_count = 0\n",
    "    nonflooded_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    # Process each unlabeled image\n",
    "    for img_name in tqdm(unlabeled_images):\n",
    "        img_path = os.path.join(unlabeled_dir, img_name)\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        img = tf.keras.preprocessing.image.load_img(img_path, target_size=IMG_SIZE)\n",
    "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img_array = img_array / 255.0  # Normalize\n",
    "        img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "        \n",
    "        # Get prediction\n",
    "        prediction = model.predict(img_array)[0]\n",
    "        confidence = max(prediction)\n",
    "        predicted_class = np.argmax(prediction)\n",
    "        \n",
    "        # Only keep high confidence predictions\n",
    "        if confidence >= confidence_threshold:\n",
    "            if predicted_class == 0:  # Non-Flooded\n",
    "                target_dir = pseudo_nonflooded_dir\n",
    "                nonflooded_count += 1\n",
    "            else:  # Flooded\n",
    "                target_dir = pseudo_flooded_dir\n",
    "                flooded_count += 1\n",
    "                \n",
    "            # Copy the image to the appropriate pseudo-label directory\n",
    "            from shutil import copy\n",
    "            copy(img_path, os.path.join(target_dir, img_name))\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "    \n",
    "    print(f\"Pseudo-labeling complete:\")\n",
    "    print(f\"  - Flooded: {flooded_count}\")\n",
    "    print(f\"  - Non-Flooded: {nonflooded_count}\")\n",
    "    print(f\"  - Skipped (low confidence): {skipped_count}\")\n",
    "    \n",
    "    return flooded_count + nonflooded_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_initial_model(model_name='inceptionv3'):\n",
    "    \"\"\"Train the initial model on labeled data\"\"\"\n",
    "    print(f\"Training initial {model_name} model on labeled data...\")\n",
    "    \n",
    "    # Set up data generators\n",
    "    train_generator, val_generator = setup_data_generators()\n",
    "    \n",
    "    # Build model based on selected architecture\n",
    "    if model_name.lower() == 'inceptionv3':\n",
    "        model = build_inceptionv3_model()\n",
    "    elif model_name.lower() == 'resnet50':\n",
    "        model = build_resnet50_model()\n",
    "    elif model_name.lower() == 'xception':\n",
    "        model = build_xception_model()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "    \n",
    "    # Set up callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            os.path.join(MODELS_DIR, f'{model_name}_initial.h5'),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            patience=10,\n",
    "            monitor='val_accuracy',\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        ),\n",
    "        TensorBoard(\n",
    "            log_dir=os.path.join(LOGS_DIR, f'{model_name}_initial_{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,  # As specified in the paper\n",
    "        epochs=EPOCHS,  # As specified in the paper\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=STEPS_PER_EPOCH // 4,  # Adjust based on validation set size\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    # Save final model\n",
    "    model.save(os.path.join(MODELS_DIR, f'{model_name}_final.h5'))\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history, model_name)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with semi-supervised learning\n",
    "def train_with_pseudo_labels(initial_model, model_name='inceptionv3'):\n",
    "    \"\"\"Train model with pseudo-labels in a semi-supervised approach\"\"\"\n",
    "    print(\"Starting semi-supervised training with pseudo-labels...\")\n",
    "    \n",
    "    # Generate pseudo-labels using the initial model\n",
    "    pseudo_label_count = generate_pseudo_labels(initial_model)\n",
    "    \n",
    "    if pseudo_label_count == 0:\n",
    "        print(\"No confident pseudo-labels generated. Skipping semi-supervised training.\")\n",
    "        return initial_model, None\n",
    "    \n",
    "    # Set up new data generator including pseudo-labeled data\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    # Load combined training data (original + pseudo-labeled)\n",
    "    combined_train_generator = train_datagen.flow_from_directory(\n",
    "        os.path.join(PREPROCESSED_DIR, 'combined_train'),\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Validation data generator\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        os.path.join(PREPROCESSED_DIR, 'train'),  # using train for validation (would need to split in real scenario)\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Create a fresh model\n",
    "    if model_name.lower() == 'inceptionv3':\n",
    "        model = build_inceptionv3_model()\n",
    "    elif model_name.lower() == 'resnet50':\n",
    "        model = build_resnet50_model()\n",
    "    elif model_name.lower() == 'xception':\n",
    "        model = build_xception_model()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "    \n",
    "    # Set up callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            os.path.join(MODELS_DIR, f'{model_name}_semisupervised.h5'),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            patience=10,\n",
    "            monitor='val_accuracy',\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=5,\n",
    "            min_lr=1e-6\n",
    "        ),\n",
    "        TensorBoard(\n",
    "            log_dir=os.path.join(LOGS_DIR, f'{model_name}_semisupervised_{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}')\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Train the model with combined data\n",
    "    history = model.fit(\n",
    "        combined_train_generator,\n",
    "        steps_per_epoch=STEPS_PER_EPOCH,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=STEPS_PER_EPOCH // 4,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    # Save final model\n",
    "    model.save(os.path.join(MODELS_DIR, f'{model_name}_semisupervised_final.h5'))\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(history, f\"{model_name}_semisupervised\")\n",
    "    \n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare combined training set with original labels and pseudo-labels\n",
    "def prepare_combined_training_set():\n",
    "    \"\"\"Prepare a combined training set with original and pseudo-labeled data\"\"\"\n",
    "    # Define paths\n",
    "    original_train = os.path.join(PREPROCESSED_DIR, 'train')\n",
    "    pseudo_labeled = os.path.join(PREPROCESSED_DIR, 'pseudo_labeled')\n",
    "    combined_train = os.path.join(PREPROCESSED_DIR, 'combined_train')\n",
    "    \n",
    "    # Create combined directory\n",
    "    os.makedirs(os.path.join(combined_train, 'Flooded'), exist_ok=True)\n",
    "    os.makedirs(os.path.join(combined_train, 'Non-Flooded'), exist_ok=True)\n",
    "    \n",
    "    # Copy original labeled data\n",
    "    import shutil\n",
    "    \n",
    "    # Copy Flooded images\n",
    "    original_flooded = os.path.join(original_train, 'Flooded')\n",
    "    combined_flooded = os.path.join(combined_train, 'Flooded')\n",
    "    for img in os.listdir(original_flooded):\n",
    "        if img.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            shutil.copy(\n",
    "                os.path.join(original_flooded, img),\n",
    "                os.path.join(combined_flooded, f\"orig_{img}\")\n",
    "            )\n",
    "    \n",
    "    # Copy Non-Flooded images\n",
    "    original_nonflooded = os.path.join(original_train, 'Non-Flooded')\n",
    "    combined_nonflooded = os.path.join(combined_train, 'Non-Flooded')\n",
    "    for img in os.listdir(original_nonflooded):\n",
    "        if img.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            shutil.copy(\n",
    "                os.path.join(original_nonflooded, img),\n",
    "                os.path.join(combined_nonflooded, f\"orig_{img}\")\n",
    "            )\n",
    "    \n",
    "    # Copy pseudo-labeled Flooded images\n",
    "    pseudo_flooded = os.path.join(pseudo_labeled, 'Flooded')\n",
    "    if os.path.exists(pseudo_flooded):\n",
    "        for img in os.listdir(pseudo_flooded):\n",
    "            if img.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                shutil.copy(\n",
    "                    os.path.join(pseudo_flooded, img),\n",
    "                    os.path.join(combined_flooded, f\"pseudo_{img}\")\n",
    "                )\n",
    "    \n",
    "    # Copy pseudo-labeled Non-Flooded images\n",
    "    pseudo_nonflooded = os.path.join(pseudo_labeled, 'Non-Flooded')\n",
    "    if os.path.exists(pseudo_nonflooded):\n",
    "        for img in os.listdir(pseudo_nonflooded):\n",
    "            if img.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                shutil.copy(\n",
    "                    os.path.join(pseudo_nonflooded, img),\n",
    "                    os.path.join(combined_nonflooded, f\"pseudo_{img}\")\n",
    "                )\n",
    "    \n",
    "    # Print statistics\n",
    "    flooded_count = len([f for f in os.listdir(combined_flooded) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    nonflooded_count = len([f for f in os.listdir(combined_nonflooded) if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    \n",
    "    print(f\"Combined training set prepared:\")\n",
    "    print(f\"  - Flooded: {flooded_count} images\")\n",
    "    print(f\"  - Non-Flooded: {nonflooded_count} images\")\n",
    "    print(f\"  - Total: {flooded_count + nonflooded_count} images\")\n",
    "    \n",
    "    return flooded_count + nonflooded_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"Plot and save training history\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='train')\n",
    "    if 'val_accuracy' in history.history:\n",
    "        plt.plot(history.history['val_accuracy'], label='validation')\n",
    "    plt.title(f'{model_name} - Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    if 'val_loss' in history.history:\n",
    "        plt.plot(history.history['val_loss'], label='validation')\n",
    "    plt.title(f'{model_name} - Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(LOGS_DIR, f'{model_name}_training_history.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset_name='validation'):\n",
    "    \"\"\"Evaluate model on validation or test set\"\"\"\n",
    "    print(f\"Evaluating model on {dataset_name} set...\")\n",
    "    \n",
    "    # Load data generator\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    # We need ground truth labels to evaluate properly\n",
    "    # For demonstration, assuming we have validation labels\n",
    "    generator = datagen.flow_from_directory(\n",
    "        os.path.join(PREPROCESSED_DIR, dataset_name),\n",
    "        target_size=IMG_SIZE,\n",
    "        batch_size=1,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = model.predict(generator, steps=len(generator))\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Get true labels\n",
    "    true_classes = generator.classes\n",
    "    \n",
    "    # Class mapping\n",
    "    class_indices = generator.class_indices\n",
    "    class_names = {v: k for k, v in class_indices.items()}\n",
    "    \n",
    "    # Print classification report\n",
    "    report = classification_report(\n",
    "        true_classes,\n",
    "        predicted_classes,\n",
    "        target_names=list(class_names.values())\n",
    "    )\n",
    "    print(report)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(true_classes, predicted_classes)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=list(class_names.values()),\n",
    "                yticklabels=list(class_names.values()))\n",
    "    plt.title(f'Confusion Matrix - {dataset_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(LOGS_DIR, f'confusion_matrix_{dataset_name}.png'))\n",
    "    \n",
    "    return report, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"FloodNet Image Classification Training\")\n",
    "    print(\"=====================================\")\n",
    "    \n",
    "    # Check if preprocessed data exists\n",
    "    if not os.path.exists(PREPROCESSED_DIR):\n",
    "        print(f\"Error: Preprocessed directory {PREPROCESSED_DIR} does not exist!\")\n",
    "        print(\"Please run the preprocessing script first.\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Choose model\n",
    "    model_name = 'inceptionv3'  # Options: 'inceptionv3', 'resnet50', 'xception'\n",
    "    print(f\"Selected model: {model_name}\")\n",
    "    \n",
    "    # Train initial model on labeled data\n",
    "    initial_model, initial_history = train_initial_model(model_name)\n",
    "    \n",
    "    # Generate pseudo-labels and prepare combined dataset\n",
    "    prepare_combined_training_set()\n",
    "    \n",
    "    # Train with semi-supervised approach\n",
    "    final_model, semisup_history = train_with_pseudo_labels(initial_model, model_name)\n",
    "    \n",
    "    # Evaluate model (requires proper validation set with labels)\n",
    "    try:\n",
    "        evaluate_model(final_model, 'validation')\n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation failed: {e}\")\n",
    "        print(\"Note: To properly evaluate the model, you need labeled validation data.\")\n",
    "    \n",
    "    print(\"\\nTraining Complete!\")\n",
    "    print(f\"Model saved to: {os.path.join(MODELS_DIR, f'{model_name}_semisupervised_final.h5')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
